{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cf8c8f",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c314d23",
   "metadata": {},
   "source": [
    "#### Set up for dataset and model\n",
    "\n",
    "Package installation, loading, and dataloaders. There's also a resnet18 model defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba145bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboardX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "## Dataloaders\n",
    "train_dataset = datasets.CIFAR10('cifar10_data/', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "test_dataset = datasets.CIFAR10('cifar10_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9892351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tp_relu(x, delta=1.):\n",
    "    ind1 = (x < -1. * delta).float()\n",
    "    ind2 = (x > delta).float()\n",
    "    return .5 * (x + delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
    "\n",
    "def tp_smoothed_relu(x, delta=1.):\n",
    "    ind1 = (x < -1. * delta).float()\n",
    "    ind2 = (x > delta).float()\n",
    "    return (x + delta) ** 2 / (4 * delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mu, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mu, self.std = mu, std\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mu) / self.std\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        return inputs\n",
    "    \n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, bn, learnable_bn, stride=1, activation='relu'):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.collect_preact = True\n",
    "        self.activation = activation\n",
    "        self.avg_preacts = []\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, affine=learnable_bn) if bn else IdentityLayer()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=not learnable_bn)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, affine=learnable_bn) if bn else IdentityLayer()\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=not learnable_bn)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=not learnable_bn)\n",
    "            )\n",
    "\n",
    "    def act_function(self, preact):\n",
    "        if self.activation == 'relu':\n",
    "            act = F.relu(preact)\n",
    "        elif self.activation[:6] == '3prelu':\n",
    "            act = tp_relu(preact, delta=float(self.activation.split('relu')[1]))\n",
    "        elif self.activation[:8] == '3psmooth':\n",
    "            act = tp_smoothed_relu(preact, delta=float(self.activation.split('smooth')[1]))\n",
    "        else:\n",
    "            assert self.activation[:8] == 'softplus'\n",
    "            beta = int(self.activation.split('softplus')[1])\n",
    "            act = F.softplus(preact, beta=beta)\n",
    "        return act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act_function(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x  # Important: using out instead of x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.act_function(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, n_cls, cuda=True, half_prec=False,\n",
    "        activation='relu', fts_before_bn=False, normal='none'):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.bn = True\n",
    "        self.learnable_bn = True  # doesn't matter if self.bn=False\n",
    "        self.in_planes = 64\n",
    "        self.avg_preact = None\n",
    "        self.activation = activation\n",
    "        self.fts_before_bn = fts_before_bn\n",
    "        if normal == 'cifar10':\n",
    "            self.mu = torch.tensor((0.4914, 0.4822, 0.4465)).view(1, 3, 1, 1)\n",
    "            self.std = torch.tensor((0.2471, 0.2435, 0.2616)).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mu = torch.tensor((0.0, 0.0, 0.0)).view(1, 3, 1, 1)\n",
    "            self.std = torch.tensor((1.0, 1.0, 1.0)).view(1, 3, 1, 1)\n",
    "            print('no input normalization')\n",
    "        if cuda:\n",
    "            self.mu = self.mu.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "        if half_prec:\n",
    "            self.mu = self.mu.half()\n",
    "            self.std = self.std.half()\n",
    "\n",
    "        self.normalize = Normalize(self.mu, self.std)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=not self.learnable_bn)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
    "        self.linear = nn.Linear(512*block.expansion, n_cls)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.bn, self.learnable_bn, stride, self.activation))\n",
    "            # layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        for layer in [*self.layer1, *self.layer2, *self.layer3, *self.layer4]:\n",
    "            layer.avg_preacts = []\n",
    "\n",
    "        out = self.normalize(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        if return_features and self.fts_before_bn:\n",
    "            return out.view(out.size(0), -1)\n",
    "        out = F.relu(self.bn(out))\n",
    "        if return_features:\n",
    "            return out.view(out.size(0), -1)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(n_cls, cuda=True, half_prec=False, activation='relu', fts_before_bn=False,\n",
    "    normal='none'):\n",
    "    #print('initializing PA RN-18 with act {}, normal {}'.format())\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], n_cls=n_cls, cuda=cuda, half_prec=half_prec,\n",
    "        activation=activation, fts_before_bn=fts_before_bn, normal=normal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f04e1",
   "metadata": {},
   "source": [
    "#### Implement the Attacks\n",
    "\n",
    "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
    "\n",
    "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec2d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_linf_untargeted(model, x, labels, k, eps, eps_step):\n",
    "    model.eval()\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    adv_x = x.clone().detach().to(device)\n",
    "    adv_x.requires_grad_(True) \n",
    "    for _ in range(k):\n",
    "        adv_x.requires_grad_(True)\n",
    "        model.zero_grad()\n",
    "        output = model(adv_x)\n",
    "        # TODO: Calculate the loss\n",
    "        loss = ce_loss(output, labels)\n",
    "        loss.backward()\n",
    "        # TODO: compute the adv_x\n",
    "        # find delta, clamp with eps\n",
    "        with torch.no_grad():\n",
    "            adv_x = adv_x + eps_step * adv_x.grad.sign()\n",
    "            delta = adv_x - x\n",
    "            delta = torch.clamp(delta, -eps, eps)\n",
    "            adv_x = torch.clamp(x + delta, min=0, max=1).detach()\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a405373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_l2_untargeted(model, x, labels, k, eps, eps_step):\n",
    "    model.eval()\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    adv_x = x.clone().detach()\n",
    "    adv_x.requires_grad_(True) \n",
    "    for _ in range(k):\n",
    "        adv_x.requires_grad_(True)\n",
    "        model.zero_grad()\n",
    "        output = model(adv_x)\n",
    "        batch_size = x.size()[0]\n",
    "        # TODO: Calculate the loss\n",
    "        loss = ce_loss(output, labels)\n",
    "        loss.backward()\n",
    "        grad = adv_x.grad.data\n",
    "        # TODO: compute the adv_x\n",
    "        # find delta, clamp with eps, project delta to the l2 ball\n",
    "        # HINT: https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgdl2.py\n",
    "        with torch.no_grad():\n",
    "            grad_norms = (\n",
    "                torch.norm(grad.view(batch_size, -1), p=2, dim=1) + 1e-10\n",
    "            )\n",
    "            adv_x = adv_x + eps_step * grad / grad_norms.view(-1, 1, 1, 1)\n",
    "            delta = adv_x - x\n",
    "            delta_norms = torch.norm(delta.view(batch_size, -1), p=2, dim=1)\n",
    "            factor = eps / delta_norms\n",
    "            factor = torch.min(factor, torch.ones_like(delta_norms))\n",
    "            delta = delta * factor.view(-1, 1, 1, 1)\n",
    "            adv_x = torch.clamp(x + delta, min=0, max=1).detach()\n",
    "\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f106a1",
   "metadata": {},
   "source": [
    "### PDG-based Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a0c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_train(\n",
    "    model, train_loader, optimizer, epoch, k, eps, attack_type=\"original\"\n",
    "):\n",
    "    model.train()\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[epoch // 2, (3 * epoch) // 4], gamma=0.1\n",
    "    )\n",
    "    if attack_type == \"original\":\n",
    "        attack_func = lambda model, x, y, k, eps, eps_step: x\n",
    "    elif attack_type == \"pgd_l2\":\n",
    "        attack_func = pgd_l2_untargeted\n",
    "    elif attack_type == \"pgd_linf\":\n",
    "        attack_func = pgd_linf_untargeted\n",
    "        \n",
    "    for e in range(epoch):\n",
    "        progress_bar = tqdm(train_loader)  # Move this inside the epoch loop\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            adv_data = attack_func(model, data, target, k, eps, eps / 4)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(adv_data)\n",
    "            loss = ce_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {e+1} | Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beee96d",
   "metadata": {},
   "source": [
    "#### Evaluate Single and Multi-Norm Robust Accuracy\n",
    "\n",
    "In this section, we evaluate the model on the Linf and L2 attacks as well as union accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38df5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_single_attack(model, attack=\"pgd_linf\", k=10, eps=0.1):\n",
    "    model.eval()\n",
    "    tot_test, tot_acc, orig_acc = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(\n",
    "        enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"\n",
    "    ):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        if attack == \"pgd_linf\":\n",
    "            x_adv = pgd_linf_untargeted(\n",
    "                model, x_batch, y_batch, k=k, eps=eps, eps_step=eps / 4\n",
    "            )\n",
    "        elif attack == \"pgd_l2\":\n",
    "            x_adv = pgd_l2_untargeted(\n",
    "                model, x_batch, y_batch, k=k, eps=eps, eps_step=eps / 4\n",
    "            )\n",
    "        else:\n",
    "            x_adv = x_batch\n",
    "\n",
    "        # get the testing accuracy and update tot_test and tot_acc\n",
    "        with torch.no_grad():\n",
    "            out = model(x_adv)\n",
    "            tot_acc += (out.argmax(dim=1) == y_batch).sum().item()\n",
    "            tot_test += x_batch.size(0)\n",
    "            orig_acc += (model(x_batch).argmax(dim=1) == y_batch).sum().item()\n",
    "\n",
    "    print(\n",
    "        \"Standard accuracy %.5lf\" % (orig_acc / tot_test),\n",
    "        \"Robust accuracy %.5lf\" % (tot_acc / tot_test),\n",
    "        f\"on {attack} attack with eps = {eps}\",\n",
    "    )\n",
    "    return orig_acc / tot_test, tot_acc / tot_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c83fe6",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea9d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with original data\n",
      "no input normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 782/782 | Loss: 1.5050: 100%|██████████| 782/782 [00:50<00:00, 15.54it/s]\n",
      "Epoch 2 | Batch 782/782 | Loss: 1.0727: 100%|██████████| 782/782 [00:51<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with pgd_linf attack and eps 0.00784313725490196\n",
      "no input normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 782/782 | Loss: 2.0554: 100%|██████████| 782/782 [07:24<00:00,  1.76it/s]\n",
      "Epoch 2 | Batch 782/782 | Loss: 1.1615: 100%|██████████| 782/782 [07:25<00:00,  1.76it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [01:25<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.48600 Robust accuracy 0.40380 on pgd_linf attack with eps = 0.00784313725490196\n",
      "Training model with pgd_linf attack and eps 0.01568627450980392\n",
      "no input normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 782/782 | Loss: 1.6302: 100%|██████████| 782/782 [07:25<00:00,  1.76it/s]\n",
      "Epoch 2 | Batch 782/782 | Loss: 1.5671: 100%|██████████| 782/782 [07:26<00:00,  1.75it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [01:25<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.47180 Robust accuracy 0.36090 on pgd_linf attack with eps = 0.01568627450980392\n",
      "Training model with pgd_linf attack and eps 0.03137254901960784\n",
      "no input normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 782/782 | Loss: 1.8665: 100%|██████████| 782/782 [07:24<00:00,  1.76it/s]\n",
      "Epoch 2 | Batch 782/782 | Loss: 2.1642: 100%|██████████| 782/782 [07:24<00:00,  1.76it/s]\n",
      "Evaluating: 100%|██████████| 157/157 [01:25<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.41770 Robust accuracy 0.28700 on pgd_linf attack with eps = 0.03137254901960784\n",
      "Results saved to training_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "attacks = [\"pgd_linf\"]\n",
    "epsilons = [2 / 255, 4 / 255, 8 / 255]\n",
    "epoch = 2\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training model with original data\")\n",
    "model = PreActResNet18(10, cuda=True, activation=\"softplus1\").to(device)\n",
    "\n",
    "adversarial_train(model, train_loader, optim.SGD(model.parameters(), lr=0.1,\n",
    "    momentum=0.9, weight_decay=5e-4), epoch=epoch, k=10, eps=8/255, attack_type='original')\n",
    "torch.save(model.state_dict(), './models/model_original.pth')\n",
    "\n",
    "for attack in attacks:\n",
    "    for eps_val in epsilons:\n",
    "        print(f\"Training model with {attack} attack and eps {eps_val}\")\n",
    "        model = PreActResNet18(10, cuda=True, activation=\"softplus1\").to(device)\n",
    "\n",
    "        adversarial_train(model, train_loader, optim.SGD(model.parameters(), lr=0.1,\n",
    "            momentum=0.9, weight_decay=5e-4), epoch=epoch, k=10, eps=eps_val, attack_type=attack)\n",
    "        orig_acc, robust_acc = test_model_on_single_attack(model, attack=attack, k=10, eps=eps_val)\n",
    "        torch.save(model.state_dict(), f'./models/model_{attack}_eps{int(eps_val * 255)}.pth')\n",
    "        results[f'{attack}_eps{int(eps_val * 255)}'] = {'orig_acc': orig_acc, 'robust_acc': robust_acc}\n",
    "\n",
    "# Save results to file\n",
    "with open('training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to training_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a64fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no input normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.51760 Robust accuracy 0.46940 on pgd_linf attack with eps = 0.00784313725490196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.51760 Robust accuracy 0.42420 on pgd_linf attack with eps = 0.01568627450980392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.51760 Robust accuracy 0.34130 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.41770 Robust accuracy 0.41110 on pgd_linf attack with eps = 0.00784313725490196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.41770 Robust accuracy 0.40380 on pgd_linf attack with eps = 0.01568627450980392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.41770 Robust accuracy 0.38830 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.47180 Robust accuracy 0.45610 on pgd_linf attack with eps = 0.00784313725490196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.47180 Robust accuracy 0.44350 on pgd_linf attack with eps = 0.01568627450980392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.47180 Robust accuracy 0.41640 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.48600 Robust accuracy 0.46430 on pgd_linf attack with eps = 0.00784313725490196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.48600 Robust accuracy 0.44370 on pgd_linf attack with eps = 0.01568627450980392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:14<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.48600 Robust accuracy 0.40360 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = PreActResNet18(10, cuda=True, activation=\"softplus1\").to(device)\n",
    "model.load_state_dict(torch.load('./models/model_original.pth'))\n",
    "for eps in epsilons:\n",
    "    test_model_on_single_attack(model, attack=\"pgd_linf\", k=1, eps=eps)\n",
    "model.load_state_dict(torch.load('./models/model_pgd_linf_eps8.pth'))\n",
    "for eps in epsilons:\n",
    "    test_model_on_single_attack(model, attack=\"pgd_linf\", k=1, eps=eps)\n",
    "model.load_state_dict(torch.load('./models/model_pgd_linf_eps4.pth'))\n",
    "for eps in epsilons:\n",
    "    test_model_on_single_attack(model, attack=\"pgd_linf\", k=1, eps=eps) \n",
    "model.load_state_dict(torch.load('./models/model_pgd_linf_eps2.pth'))\n",
    "for eps in epsilons:\n",
    "    test_model_on_single_attack(model, attack=\"pgd_linf\", k=1, eps=eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998981a",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d727101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "from bound_propagation import BoundModelFactory, HyperRectangle\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__(\n",
    "            nn.Linear(28 * 28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x = x.view(x.size(0), -1)\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "def get_logit(bounds, y):\n",
    "    batch_size = y.size(0)\n",
    "    logit = bounds.upper.clone()\n",
    "    logit[torch.arange(batch_size), y] = bounds.lower[torch.arange(batch_size), y]\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ibp(model, lr=1e-3, num_epochs=100, k_final=0.5, epsilon_target=0.1):\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    train_data = datasets.MNIST(\n",
    "        \"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(train_data, batch_size=100, shuffle=True, num_workers=4)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    warm_up = int(2 / 60 * total_steps)\n",
    "    ramp_up = int(10 / 60 * total_steps)\n",
    "    lr_decay_steps = [int(15 / 60 * total_steps), int(25 / 60 * total_steps)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    current_step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            if current_step in lr_decay_steps:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] *= 0.1\n",
    "\n",
    "            if current_step < warm_up:\n",
    "                lr_scale = current_step / warm_up\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = lr * lr_scale\n",
    "\n",
    "            k = max(\n",
    "                1.0\n",
    "                - (1.0 - k_final)\n",
    "                * max(0, current_step - warm_up)\n",
    "                / (total_steps - warm_up),\n",
    "                k_final,\n",
    "            )\n",
    "\n",
    "            if current_step < ramp_up:\n",
    "                eps_train = epsilon_target * (current_step / ramp_up)\n",
    "            else:\n",
    "                eps_train = epsilon_target\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            y_hat = model(data)\n",
    "            ce_loss = criterion(y_hat, target)\n",
    "\n",
    "            bounds = model.ibp(HyperRectangle.from_eps(data.view(data.size(0), -1), eps_train))\n",
    "            adv_logit = get_logit(bounds, target)\n",
    "            robust_loss = criterion(adv_logit, target)\n",
    "\n",
    "            loss = k * ce_loss + (1 - k) * robust_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_step += 1\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'CE_Loss': f'{ce_loss.item():.4f}',\n",
    "                'Rob_Loss': f'{robust_loss.item():.4f}',\n",
    "                'k': f'{k:.3f}',\n",
    "                'eps': f'{eps_train:.3f}'\n",
    "            })\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"IBP Training completed in {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21d1ba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 600/600 [00:17<00:00, 34.47it/s, Loss=0.3648, CE_Loss=0.2207, Rob_Loss=17.0253, k=0.991, eps=0.030]\n",
      "Epoch 2: 100%|██████████| 600/600 [00:17<00:00, 34.99it/s, Loss=0.2595, CE_Loss=0.1562, Rob_Loss=3.1550, k=0.966, eps=0.060] \n",
      "Epoch 3: 100%|██████████| 600/600 [00:17<00:00, 33.47it/s, Loss=0.5122, CE_Loss=0.3663, Rob_Loss=2.7865, k=0.940, eps=0.090]\n",
      "Epoch 4: 100%|██████████| 600/600 [00:19<00:00, 30.35it/s, Loss=0.3750, CE_Loss=0.2508, Rob_Loss=1.6917, k=0.914, eps=0.100]\n",
      "Epoch 5: 100%|██████████| 600/600 [00:20<00:00, 29.98it/s, Loss=0.2873, CE_Loss=0.1811, Rob_Loss=1.1294, k=0.888, eps=0.100]\n",
      "Epoch 6: 100%|██████████| 600/600 [00:20<00:00, 29.38it/s, Loss=0.2747, CE_Loss=0.1692, Rob_Loss=0.9339, k=0.862, eps=0.100]\n",
      "Epoch 7: 100%|██████████| 600/600 [00:20<00:00, 28.95it/s, Loss=0.2333, CE_Loss=0.1322, Rob_Loss=0.7494, k=0.836, eps=0.100]\n",
      "Epoch 8: 100%|██████████| 600/600 [00:19<00:00, 31.34it/s, Loss=0.1672, CE_Loss=0.0714, Rob_Loss=0.5766, k=0.810, eps=0.100]\n",
      "Epoch 9: 100%|██████████| 600/600 [00:17<00:00, 33.72it/s, Loss=0.2436, CE_Loss=0.1210, Rob_Loss=0.6901, k=0.785, eps=0.100]\n",
      "Epoch 10: 100%|██████████| 600/600 [00:17<00:00, 33.73it/s, Loss=0.2063, CE_Loss=0.0813, Rob_Loss=0.5993, k=0.759, eps=0.100]\n",
      "Epoch 11: 100%|██████████| 600/600 [00:18<00:00, 32.38it/s, Loss=0.3080, CE_Loss=0.1437, Rob_Loss=0.7587, k=0.733, eps=0.100]\n",
      "Epoch 12: 100%|██████████| 600/600 [00:19<00:00, 30.80it/s, Loss=0.3018, CE_Loss=0.1293, Rob_Loss=0.7178, k=0.707, eps=0.100]\n",
      "Epoch 13: 100%|██████████| 600/600 [00:18<00:00, 33.02it/s, Loss=0.3870, CE_Loss=0.1823, Rob_Loss=0.8242, k=0.681, eps=0.100]\n",
      "Epoch 14: 100%|██████████| 600/600 [00:18<00:00, 32.86it/s, Loss=0.2579, CE_Loss=0.1080, Rob_Loss=0.5430, k=0.655, eps=0.100]\n",
      "Epoch 15: 100%|██████████| 600/600 [00:18<00:00, 33.22it/s, Loss=0.4283, CE_Loss=0.1929, Rob_Loss=0.8282, k=0.629, eps=0.100]\n",
      "Epoch 16: 100%|██████████| 600/600 [00:17<00:00, 33.87it/s, Loss=0.3797, CE_Loss=0.1062, Rob_Loss=0.7961, k=0.603, eps=0.100]\n",
      "Epoch 17: 100%|██████████| 600/600 [00:17<00:00, 33.97it/s, Loss=0.4838, CE_Loss=0.1904, Rob_Loss=0.8850, k=0.578, eps=0.100]\n",
      "Epoch 18: 100%|██████████| 600/600 [00:17<00:00, 33.73it/s, Loss=0.2437, CE_Loss=0.0617, Rob_Loss=0.4678, k=0.552, eps=0.100]\n",
      "Epoch 19: 100%|██████████| 600/600 [00:17<00:00, 33.42it/s, Loss=0.2519, CE_Loss=0.0892, Rob_Loss=0.4323, k=0.526, eps=0.100]\n",
      "Epoch 20: 100%|██████████| 600/600 [00:18<00:00, 32.28it/s, Loss=0.5480, CE_Loss=0.2017, Rob_Loss=0.8943, k=0.500, eps=0.100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBP Training completed in 370.46 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370.4572105407715"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleNet()\n",
    "bound_model = BoundModelFactory().build(model)\n",
    "bound_model.to(device)\n",
    "train_ibp(bound_model, lr=1e-3, num_epochs=20, k_final=0.5, epsilon_target=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb72d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_standard(model, lr=1e-3, num_epochs=100):\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    train_data = datasets.MNIST(\n",
    "        \"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(train_data, batch_size=100, shuffle=True, num_workers=4)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            y_hat = model(data)\n",
    "            ce_loss = criterion(y_hat, target)\n",
    "\n",
    "            loss = ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"Loss\": f\"{loss.item():.4f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Standard Training completed in {training_time:.2f} seconds\")\n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08307ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 600/600 [00:10<00:00, 55.62it/s, Loss=0.3106] \n",
      "Epoch 2: 100%|██████████| 600/600 [00:10<00:00, 54.94it/s, Loss=0.0835] \n",
      "Epoch 3: 100%|██████████| 600/600 [00:11<00:00, 54.26it/s, Loss=0.0896] \n",
      "Epoch 4: 100%|██████████| 600/600 [00:10<00:00, 56.82it/s, Loss=0.1414] \n",
      "Epoch 5: 100%|██████████| 600/600 [00:10<00:00, 55.10it/s, Loss=0.0280] \n",
      "Epoch 6: 100%|██████████| 600/600 [00:10<00:00, 55.55it/s, Loss=0.1236] \n",
      "Epoch 7: 100%|██████████| 600/600 [00:10<00:00, 55.22it/s, Loss=0.1043] \n",
      "Epoch 8: 100%|██████████| 600/600 [00:10<00:00, 54.97it/s, Loss=0.1175] \n",
      "Epoch 9: 100%|██████████| 600/600 [00:10<00:00, 57.94it/s, Loss=0.0289] \n",
      "Epoch 10: 100%|██████████| 600/600 [00:10<00:00, 56.32it/s, Loss=0.0843] \n",
      "Epoch 11: 100%|██████████| 600/600 [00:10<00:00, 57.72it/s, Loss=0.1000] \n",
      "Epoch 12: 100%|██████████| 600/600 [00:11<00:00, 52.17it/s, Loss=0.0112] \n",
      "Epoch 13: 100%|██████████| 600/600 [00:11<00:00, 50.18it/s, Loss=0.0295] \n",
      "Epoch 14: 100%|██████████| 600/600 [00:12<00:00, 49.92it/s, Loss=0.0278] \n",
      "Epoch 15: 100%|██████████| 600/600 [00:10<00:00, 55.26it/s, Loss=0.0681] \n",
      "Epoch 16: 100%|██████████| 600/600 [00:10<00:00, 55.50it/s, Loss=0.0156] \n",
      "Epoch 17: 100%|██████████| 600/600 [00:10<00:00, 54.93it/s, Loss=0.0309] \n",
      "Epoch 18: 100%|██████████| 600/600 [00:10<00:00, 56.61it/s, Loss=0.0521] \n",
      "Epoch 19: 100%|██████████| 600/600 [00:10<00:00, 55.78it/s, Loss=0.0117] \n",
      "Epoch 20: 100%|██████████| 600/600 [00:10<00:00, 57.30it/s, Loss=0.0227] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Training completed in 218.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "218.17817997932434"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_model = SimpleNet().to(device)\n",
    "train_standard(standard_model, lr=1e-3, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68c13146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_analysis(net, epsilons):\n",
    "\n",
    "    mnist_test_loader = DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for e in epsilons:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            progress_bar = tqdm(mnist_test_loader, desc=f\"Epsilon {e:.3f}\")\n",
    "            for imgs, labels in progress_bar:\n",
    "                box = HyperRectangle.from_eps(imgs.view(imgs.size(0), -1), e)\n",
    "                out = net.crown_ibp(box, alpha=True).concretize()\n",
    "                lo, hi = out.lower, out.upper\n",
    "                labs = labels.cpu().numpy()\n",
    "                bad_mask = torch.ones_like(hi, dtype=torch.bool, device=hi.device)\n",
    "                for idx, lab in enumerate(labs):\n",
    "                    bad_mask[idx, lab] = False\n",
    "                lo_true = lo[range(len(labels)), labs]\n",
    "                hi_bad = hi.masked_select(bad_mask).view(len(labels), -1)\n",
    "                correct += (lo_true > hi_bad.max(dim=1).values).sum().item()\n",
    "                total += len(labels)\n",
    "            print(f\"Epsilon: {e}, Robust Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbd1230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bound_model_on_single_attack(model, attack=\"pgd_linf\", k=10, eps=0.1):\n",
    "\n",
    "    bound_model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    tot_test, tot_acc, orig_acc = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(\n",
    "        enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"\n",
    "    ):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        if attack == \"pgd_linf\":\n",
    "            x_adv = pgd_linf_untargeted(\n",
    "                model, x_batch, y_batch, k=k, eps=eps, eps_step=eps / 4\n",
    "            )\n",
    "        elif attack == \"pgd_l2\":\n",
    "            x_adv = pgd_l2_untargeted(\n",
    "                model, x_batch, y_batch, k=k, eps=eps, eps_step=eps / 4\n",
    "            )\n",
    "        else:\n",
    "            x_adv = x_batch\n",
    "\n",
    "        # get the testing accuracy and update tot_test and tot_acc\n",
    "        with torch.no_grad():\n",
    "            out = model(x_adv)\n",
    "            tot_acc += (out.argmax(dim=1) == y_batch).sum().item()\n",
    "            tot_test += x_batch.size(0)\n",
    "            orig_acc += (model(x_batch).argmax(dim=1) == y_batch).sum().item()\n",
    "\n",
    "    print(\n",
    "        \"Standard accuracy %.5lf\" % (orig_acc / tot_test),\n",
    "        \"Robust accuracy %.5lf\" % (tot_acc / tot_test),\n",
    "        f\"on {attack} attack with eps = {eps}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3959a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10000/10000 [04:14<00:00, 39.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.95970 Robust accuracy 0.83280 on pgd_linf attack with eps = 0.1\n"
     ]
    }
   ],
   "source": [
    "test_bound_model_on_single_attack(bound_model, attack=\"pgd_linf\", k=10, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97492219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.010: 100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.01, Robust Accuracy: 95.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.020: 100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.020000000000000004, Robust Accuracy: 95.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.030: 100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.030000000000000006, Robust Accuracy: 94.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.040: 100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.04000000000000001, Robust Accuracy: 93.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.050: 100%|██████████| 100/100 [01:00<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.05000000000000001, Robust Accuracy: 92.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.060: 100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.06000000000000001, Robust Accuracy: 91.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.070: 100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.07, Robust Accuracy: 90.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.080: 100%|██████████| 100/100 [01:01<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.08, Robust Accuracy: 89.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.090: 100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.09000000000000001, Robust Accuracy: 87.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilon 0.100: 100%|██████████| 100/100 [01:01<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1, Robust Accuracy: 85.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "interval_analysis(bound_model.to('cpu'), epsilons=np.linspace(0.01, 0.1, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
