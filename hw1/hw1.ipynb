{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caefce47",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d6438a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class:  2\n",
      "New Class:  0\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# fix seed so that random initialization always performs the same \n",
    "torch.manual_seed(13)\n",
    "\n",
    "\n",
    "# create the model N as described in the question\n",
    "N = nn.Sequential(nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 3, bias=False))\n",
    "\n",
    "# random input\n",
    "x = torch.rand((1,10)) # the first dimension is the batch size; the following dimensions the actual dimension of the data\n",
    "x.requires_grad_() # this is required so we can compute the gradient w.r.t x\n",
    "\n",
    "t = 0 # target class\n",
    "\n",
    "epsReal = 0.5  #depending on your data this might be large or small\n",
    "eps = epsReal - 1e-7 # small constant to offset floating-point erros\n",
    "\n",
    "# The network N classfies x as belonging to class 2\n",
    "original_class = N(x).argmax(dim=1).item()  # TO LEARN: make sure you understand this expression\n",
    "print(\"Original Class: \", original_class)\n",
    "assert(original_class == 2)\n",
    "\n",
    "# compute gradient\n",
    "# note that CrossEntropyLoss() combines the cross-entropy loss and an implicit softmax function\n",
    "L = nn.CrossEntropyLoss()\n",
    "loss = L(N(x), torch.tensor([t], dtype=torch.long)) # TO LEARN: make sure you understand this line\n",
    "loss.backward()\n",
    "\n",
    "# your code here\n",
    "# adv_x should be computed from x according to the fgsm-style perturbation such that the new class of xBar is the target class t above\n",
    "# hint: you can compute the gradient of the loss w.r.t to x as x.grad\n",
    "adv_x = x - eps * x.grad.sign()\n",
    "\n",
    "new_class = N(adv_x).argmax(dim=1).item()\n",
    "print(\"New Class: \", new_class)\n",
    "assert(new_class == t)\n",
    "# it is not enough that adv_x is classified as t. We also need to make sure it is 'close' to the original x. \n",
    "print(torch.norm((x-adv_x),  p=float('inf')).data)\n",
    "assert( torch.norm((x-adv_x), p=float('inf')) <= epsReal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338cf91",
   "metadata": {},
   "source": [
    "Iteratively update adv_x using a smaller step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b77c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class:  2\n",
      "New Class:  1\n",
      "tensor(0.8207)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# fix seed so that random initialization always performs the same\n",
    "torch.manual_seed(13)\n",
    "\n",
    "\n",
    "# create the model N as described in the question\n",
    "N = nn.Sequential(nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 3, bias=False))\n",
    "\n",
    "# random input\n",
    "x = torch.rand((1,10)) # the first dimension is the batch size; the following dimensions the actual dimension of the data\n",
    "x.requires_grad_() # this is required so we can compute the gradient w.r.t x\n",
    "\n",
    "t = 1 # target class\n",
    "\n",
    "\n",
    "alpha = 0.01 # step size\n",
    "epsReal = 0.9  #depending on your data this might be large or small\n",
    "eps = epsReal - 1e-7 # small constant to offset floating-point erros\n",
    "\n",
    "# The network N classfies x as belonging to class 2\n",
    "original_class = N(x).argmax(dim=1).item()  # TO LEARN: make sure you understand this expression\n",
    "print(\"Original Class: \", original_class)\n",
    "assert(original_class == 2)\n",
    "\n",
    "new_class = original_class\n",
    "adv_x = x.clone().detach()\n",
    "adv_x.requires_grad_()\n",
    "while new_class!=t:\n",
    "    adv_x.grad=None\n",
    "    # compute gradient\n",
    "    # note that CrossEntropyLoss() combines the cross-entropy loss and an implicit softmax function\n",
    "    L = nn.CrossEntropyLoss()\n",
    "    loss = L(N(adv_x), torch.tensor([t], dtype=torch.long)) # TO LEARN: make sure you understand this line\n",
    "    loss.backward()\n",
    "\n",
    "    # your code here\n",
    "    # adv_x should be computed from x according to the fgsm-style perturbation such that the new class of xBar is the target class t above\n",
    "    # hint: you can compute the gradient of the loss w.r.t to x as x.grad\n",
    "    adv_x = adv_x - alpha * adv_x.grad\n",
    "    adv_x = adv_x.clamp(x - eps, x + eps).detach().requires_grad_()\n",
    "    new_class = N(adv_x).argmax(dim=1).item()\n",
    "\n",
    "print(\"New Class: \", new_class)\n",
    "assert(new_class == t)\n",
    "# it is not enough that adv_x is classified as t. We also need to make sure it is 'close' to the original x.\n",
    "print(torch.norm((x-adv_x),  p=float('inf')).data)\n",
    "assert( torch.norm((x-adv_x), p=float('inf')) <= epsReal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2919e9",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05526301",
   "metadata": {},
   "source": [
    "#### Set up for dataset and model\n",
    "\n",
    "Package installation, loading, and dataloaders. There's also a resnet18 model defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18235ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboardX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "## Dataloaders\n",
    "# train_dataset = datasets.CIFAR10('cifar10_data/', train=True, download=True, transform=transforms.Compose(\n",
    "#     [transforms.ToTensor()]\n",
    "# ))\n",
    "test_dataset = datasets.CIFAR10('cifar10_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6aaa4e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no input normalization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (normalize): Normalize()\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tp_relu(x, delta=1.):\n",
    "    ind1 = (x < -1. * delta).float()\n",
    "    ind2 = (x > delta).float()\n",
    "    return .5 * (x + delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
    "\n",
    "def tp_smoothed_relu(x, delta=1.):\n",
    "    ind1 = (x < -1. * delta).float()\n",
    "    ind2 = (x > delta).float()\n",
    "    return (x + delta) ** 2 / (4 * delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mu, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mu, self.std = mu, std\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mu) / self.std\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        return inputs\n",
    "    \n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, bn, learnable_bn, stride=1, activation='relu'):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.collect_preact = True\n",
    "        self.activation = activation\n",
    "        self.avg_preacts = []\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, affine=learnable_bn) if bn else IdentityLayer()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=not learnable_bn)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, affine=learnable_bn) if bn else IdentityLayer()\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=not learnable_bn)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=not learnable_bn)\n",
    "            )\n",
    "\n",
    "    def act_function(self, preact):\n",
    "        if self.activation == 'relu':\n",
    "            act = F.relu(preact)\n",
    "        elif self.activation[:6] == '3prelu':\n",
    "            act = tp_relu(preact, delta=float(self.activation.split('relu')[1]))\n",
    "        elif self.activation[:8] == '3psmooth':\n",
    "            act = tp_smoothed_relu(preact, delta=float(self.activation.split('smooth')[1]))\n",
    "        else:\n",
    "            assert self.activation[:8] == 'softplus'\n",
    "            beta = int(self.activation.split('softplus')[1])\n",
    "            act = F.softplus(preact, beta=beta)\n",
    "        return act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act_function(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x  # Important: using out instead of x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.act_function(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, n_cls, cuda=True, half_prec=False,\n",
    "        activation='relu', fts_before_bn=False, normal='none'):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.bn = True\n",
    "        self.learnable_bn = True  # doesn't matter if self.bn=False\n",
    "        self.in_planes = 64\n",
    "        self.avg_preact = None\n",
    "        self.activation = activation\n",
    "        self.fts_before_bn = fts_before_bn\n",
    "        if normal == 'cifar10':\n",
    "            self.mu = torch.tensor((0.4914, 0.4822, 0.4465)).view(1, 3, 1, 1)\n",
    "            self.std = torch.tensor((0.2471, 0.2435, 0.2616)).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mu = torch.tensor((0.0, 0.0, 0.0)).view(1, 3, 1, 1)\n",
    "            self.std = torch.tensor((1.0, 1.0, 1.0)).view(1, 3, 1, 1)\n",
    "            print('no input normalization')\n",
    "        if cuda:\n",
    "            self.mu = self.mu.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "        if half_prec:\n",
    "            self.mu = self.mu.half()\n",
    "            self.std = self.std.half()\n",
    "\n",
    "        self.normalize = Normalize(self.mu, self.std)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=not self.learnable_bn)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
    "        self.linear = nn.Linear(512*block.expansion, n_cls)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.bn, self.learnable_bn, stride, self.activation))\n",
    "            # layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        for layer in [*self.layer1, *self.layer2, *self.layer3, *self.layer4]:\n",
    "            layer.avg_preacts = []\n",
    "\n",
    "        out = self.normalize(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        if return_features and self.fts_before_bn:\n",
    "            return out.view(out.size(0), -1)\n",
    "        out = F.relu(self.bn(out))\n",
    "        if return_features:\n",
    "            return out.view(out.size(0), -1)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(n_cls, cuda=True, half_prec=False, activation='relu', fts_before_bn=False,\n",
    "    normal='none'):\n",
    "    #print('initializing PA RN-18 with act {}, normal {}'.format())\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], n_cls=n_cls, cuda=cuda, half_prec=half_prec,\n",
    "        activation=activation, fts_before_bn=fts_before_bn, normal=normal)\n",
    "\n",
    "\n",
    "# intialize the model\n",
    "model = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918ae46",
   "metadata": {},
   "source": [
    "#### Implement the Attacks\n",
    "\n",
    "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
    "\n",
    "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7fd22f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_linf_untargeted(model, x, labels, k, eps, eps_step):\n",
    "    model.eval()\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    adv_x = x.clone().detach().to(device)\n",
    "    adv_x.requires_grad_(True) \n",
    "    for _ in range(k):\n",
    "        adv_x.requires_grad_(True)\n",
    "        model.zero_grad()\n",
    "        output = model(adv_x)\n",
    "        # TODO: Calculate the loss\n",
    "        loss = ce_loss(output, labels)\n",
    "        loss.backward()\n",
    "        # TODO: compute the adv_x\n",
    "        # find delta, clamp with eps\n",
    "        with torch.no_grad():\n",
    "            adv_x = adv_x + eps_step * adv_x.grad.sign()\n",
    "            delta = adv_x - x\n",
    "            delta = torch.clamp(delta, -eps, eps)\n",
    "            adv_x = torch.clamp(x + delta, min=0, max=1).detach()\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36213271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_l2_untargeted(model, x, labels, k, eps, eps_step):\n",
    "    model.eval()\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    adv_x = x.clone().detach()\n",
    "    adv_x.requires_grad_(True) \n",
    "    for _ in range(k):\n",
    "        adv_x.requires_grad_(True)\n",
    "        model.zero_grad()\n",
    "        output = model(adv_x)\n",
    "        batch_size = x.size()[0]\n",
    "        # TODO: Calculate the loss\n",
    "        loss = ce_loss(output, labels)\n",
    "        loss.backward()\n",
    "        grad = adv_x.grad.data\n",
    "        # TODO: compute the adv_x\n",
    "        # find delta, clamp with eps, project delta to the l2 ball\n",
    "        # HINT: https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgdl2.py\n",
    "        with torch.no_grad():\n",
    "            grad_norms = (\n",
    "                torch.norm(grad.view(batch_size, -1), p=2, dim=1) + 1e-10\n",
    "            )\n",
    "            adv_x = adv_x + eps_step * grad / grad_norms.view(-1, 1, 1, 1)\n",
    "            delta = adv_x - x\n",
    "            delta_norms = torch.norm(delta.view(batch_size, -1), p=2, dim=1)\n",
    "            factor = eps / delta_norms\n",
    "            factor = torch.min(factor, torch.ones_like(delta_norms))\n",
    "            delta = delta * factor.view(-1, 1, 1, 1)\n",
    "            adv_x = torch.clamp(x + delta, min=0, max=1).detach()\n",
    "\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8775",
   "metadata": {},
   "source": [
    "#### Evaluate Single and Multi-Norm Robust Accuracy\n",
    "\n",
    "In this section, we evaluate the model on the Linf and L2 attacks as well as union accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27530f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_single_attack(model, attack='pgd_linf', eps=0.1):\n",
    "    model.eval()\n",
    "    tot_test, tot_acc, orig_acc = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        if attack == 'pgd_linf':\n",
    "            # TODO: get x_adv untargeted pgd linf with eps, and eps_step=eps/4\n",
    "            x_adv = pgd_linf_untargeted(model, x_batch, y_batch, k=10, eps=eps, eps_step=eps/4)\n",
    "        elif attack == 'pgd_l2':\n",
    "            # TODO: get x_adv untargeted pgd l2 with eps, and eps_step=eps/4\n",
    "            x_adv = pgd_l2_untargeted(model, x_batch, y_batch, k=10, eps=eps, eps_step=eps/4)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # get the testing accuracy and update tot_test and tot_acc\n",
    "        with torch.no_grad():\n",
    "            out = model(x_adv)\n",
    "            tot_acc += (out.argmax(dim=1) == y_batch).sum().item()\n",
    "            tot_test += x_batch.size(0)\n",
    "            orig_acc += (model(x_batch).argmax(dim=1) == y_batch).sum().item()\n",
    "\n",
    "    print('Standard accuracy %.5lf' % (orig_acc/tot_test), 'Robust accuracy %.5lf' % (tot_acc/tot_test), f'on {attack} attack with eps = {eps}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf1f41",
   "metadata": {},
   "source": [
    "## Single-Norm Robust Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75f26161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [01:24<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.82800 Robust accuracy 0.51200 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [01:25<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.88750 Robust accuracy 0.30860 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [01:26<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.81190 Robust accuracy 0.49740 on pgd_linf attack with eps = 0.03137254901960784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Linf attack with different models with eps = 8/255\n",
    "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
    "# Evaluate on Linf attack with model 1 with eps = 8/255\n",
    "test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
    "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
    "# Evaluate on Linf attack with model 2 with eps = 8/255\n",
    "test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)\n",
    "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
    "# Evaluate on Linf attack with model 3 with eps = 8/255\n",
    "test_model_on_single_attack(model, attack='pgd_linf', eps=8/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "973d6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [01:26<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.82800 Robust accuracy 0.48820 on pgd_l2 attack with eps = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [01:26<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.88750 Robust accuracy 0.54390 on pgd_l2 attack with eps = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [01:26<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.81190 Robust accuracy 0.59660 on pgd_l2 attack with eps = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on L2 attack with different models with eps = 0.75\n",
    "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
    "# Evaluate on L2 attack with model 1 with eps = 0.75\n",
    "test_model_on_single_attack(model, attack=\"pgd_l2\", eps=0.75)\n",
    "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
    "# Evaluate on L2 attack with model 2 with eps = 0.75\n",
    "test_model_on_single_attack(model, attack=\"pgd_l2\", eps=0.75)\n",
    "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
    "# Evaluate on L2 attack with model 3 with eps = 0.75\n",
    "test_model_on_single_attack(model, attack=\"pgd_l2\", eps=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c275f68",
   "metadata": {},
   "source": [
    "## Multi-Norm Robust Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2911ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75):\n",
    "    model.eval()\n",
    "    tot_test, tot_acc, orig_acc = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        # TODO: get x_adv_linf and x_adv_l2 untargeted pgd linf and l2 with eps, and eps_step=eps/4\n",
    "        x_adv_linf = pgd_linf_untargeted(model, x_batch, y_batch, k=10, eps=eps_linf, eps_step=eps_linf/4)\n",
    "        x_adv_l2 = pgd_l2_untargeted(model, x_batch, y_batch, k=10, eps=eps_l2, eps_step=eps_l2/4)\n",
    "        ## calculate union accuracy: correct only if both attacks are correct\n",
    "        with torch.no_grad():\n",
    "            out = model(x_adv_linf)\n",
    "            pred_linf = torch.max(out, dim=1)[1]\n",
    "            out = model(x_adv_l2)\n",
    "            pred_l2 = torch.max(out, dim=1)[1]\n",
    "\n",
    "            orig_acc += (model(x_batch).argmax(dim=1) == y_batch).sum().item()\n",
    "            # TODO: get the testing accuracy with multi-norm robustness and update tot_test and tot_acc\n",
    "            tot_acc += ((pred_linf == y_batch) & (pred_l2 == y_batch)).sum().item()\n",
    "            tot_test += x_batch.size(0)\n",
    "\n",
    "    print(\n",
    "        \"Standard accuracy %.5lf\" % (orig_acc / tot_test),\n",
    "        \"Robust accuracy %.5lf\" % (tot_acc / tot_test),\n",
    "        f\"on multi attacks\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f501af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:48<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.82800 Robust accuracy 0.47040 on multi attacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:48<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.88750 Robust accuracy 0.30860 on multi attacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:48<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy 0.81190 Robust accuracy 0.49730 on multi attacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on multi-norm attacks with different models with eps_linf = 8./255, eps_l2 = 0.75\n",
    "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
    "# Evaluate on multi attacks with model 1\n",
    "test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75)\n",
    "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
    "# Evaluate on multi attacks with model 2\n",
    "test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75)\n",
    "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
    "# Evaluate on multi attacks with model 3\n",
    "test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
